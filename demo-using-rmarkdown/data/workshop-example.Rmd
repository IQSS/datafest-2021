---
title: "DataFest Rmd example"
author: "DataFest 2021 team"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_chunk$set(echo = TRUE, fig.width = 12, fig.height = 12)
```

## Project details
The dataset we are working with here represents observations of COVID-19 cases over 2020 in the US (50 states and D.C.).

In this report we present the code used to download the data, the data clean up and visualizations to represent this information in an accessible manner.

## Setup

### Load Libraries

We will need several packages for data cleanup and visualization. 

```{r}
#Install the packages we need for visualization. 
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if(length(new.pkg)) install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

packages <- c("viridis", "glmmTMB",  "effects", "dataverse", "sf", "remotes",
              "leaflet", "mapview", "htmltools", "htmlwidgets", "tigris",   
              "lubridate", "DHARMa", "tidycensus", "tidyverse", "tidymodels")
ipak(packages)
```

### Download Data

First, we need to get the digital object identifier for the Covid dataset in Dataverse, followed by retreiving the contents of the dataset. Thedownloaded dataset has a lot of information, so we need to selectively extract the data we need.

```{r}
# get the digital object identifier for the Dataverse dataset
DOI <- "doi:10.7910/DVN/HIDLTK"

# retrieve the contents of the dataset
covid <- get_dataset(DOI)

#Dataset has multiple files, so let's get the files we need.
# get data file for COVID-19 cases
US_cases_file <- get_file("us_state_confirmed_case.tab", dataset = DOI)
# convert raw vector to dataframe
US_cases <- read_csv(US_cases_file)
```

### Clean up data

We need to clean up the data a little bit. 

* Select only the columns we are interested in
* Rename the columns to have more intuitive names
* Change the date format to be more readable in a new column
* Add new columns with freshly calculate stats that are tidied up so we can use them for visualization easily

```{r}
#Reformat, clean, and more intuitively name the data
US_cases_long <- US_cases %>%
  # select columns of interest
  select(fips, NAME, POP10, matches("^\\d")) %>% 
  # rename some columns
  rename(GEOID = fips, state = NAME, pop_count_2010 = POP10) %>%
  # reshape to long format for dates
  pivot_longer(cols = grep("^\\d", colnames(.), value = TRUE), 
               names_to = "date", values_to = "cases_cum") %>%
  # create new derived time variables from dates 
  mutate(date = ymd(date), # year-month-day format
         day_of_year = yday(date),
         week_of_year = week(date),
         month = month(date)) %>% 
  group_by(state) %>% 
  # create cases counts
  mutate(cases_count = cases_cum - lag(cases_cum, default = 0),
         # tidy-up negative counts
         cases_count_pos = ifelse(cases_count < 0, 0, cases_count),
         # create cases rates
         cases_rate_100K = (cases_count_pos / pop_count_2010) * 1e5,
         cases_cum_rate_100K = (cases_cum / pop_count_2010) * 1e5)
```

### Aggregate weekly numbers by state 

Finally, we want to subest the dataset and aggregate information such that each week's counts are aggregated for visualizing the data readily.

```{r}
# aggregate to weekly level (for later modeling)
US_cases_long_week <- US_cases_long %>%
  group_by(GEOID, state, week_of_year) %>%
  summarize(pop_count_2010 = mean(pop_count_2010),
            cases_count_pos = sum(cases_count_pos), 
            cases_rate_100K = sum(cases_rate_100K)) %>% 
  drop_na()
  ```

## Plots by state

### Visualize the counts

```{r}
# create line graphs of covid cases rates for each state
ggplot(US_cases_long, aes(x = date, y = cases_rate_100K)) +
  geom_line() +
  facet_wrap(c("state"), ncol = 10, scales = "fixed") +
  theme_classic() +
  theme(axis.title.x=element_blank(), #because these graphs are too small to see x-axis labels
        axis.text.x=element_blank())
```

### Visualize cumulative numbers

```{r}
# line graphs of cumulative covid cases rates for each state
ggplot(US_cases_long, aes(x = date, y = cases_cum_rate_100K)) +
  geom_line() +
  facet_wrap(~ state, scales = "fixed") +
  theme_classic() +
  theme(axis.title.x=element_blank(), #because these graphs are too small to see x-axis labels
      axis.text.x=element_blank())
```

## Using maps for visualization

### Basic map

```{r}
#Start by downloading state-level census geographies
us_state_geo <- tigris::states(class = "sf", cb = TRUE) %>%
  # rename `NAME` variable to `state`
  rename(state = NAME)
kable(us_state_geo)

kable(US_cases_long_week)
```

```{r}
# merge weekly COVID-19 cases with spatial data
US_cases_long_week_spatial <- us_state_geo %>% 
  left_join(US_cases_long_week, by = c("state")) %>% 
  filter( state != "Alaska" & state != "Hawaii") 

kable(US_cases_long_week_spatial)
```

```{r}
US_cases_long_week_spatial %>% 
  # subset data for only latest week
  filter(week_of_year == max(week_of_year, na.rm = TRUE)) %>% 
  # map starts here
  ggplot(aes(fill = cases_rate_100K, color = cases_rate_100K)) +
  geom_sf() +
  coord_sf(crs = 5070, datum = NA) +
  scale_fill_viridis(direction = 1, name = "Case rate\n(per 100K population)") + 
  #scale_color_viridis(direction = -1, name = "Case rate\n(per 100K population)") +
  labs(title = "COVID-19 case rates for last week",
       caption = "Data Sources: Harvard Dataverse, 2020; U.S. Census Bureau, 2019")
```

### Interactive map

```{r}
# set some options for the graph
mapviewOptions(fgb = FALSE, # set to FALSE to embed data directly into the HTML
               leafletWidth = 800,
               legend.pos = "bottomright")

# create map
USmap <- US_cases_long_week_spatial %>% 
  # subset data for only latest week
  filter(week_of_year == max(week_of_year, na.rm = TRUE)) %>%
  # map starts here
  mapview(zcol = "cases_rate_100K", layer.name = "Case rates (per 100K)")

# print map
USmap@map
```

## Final thoughts
